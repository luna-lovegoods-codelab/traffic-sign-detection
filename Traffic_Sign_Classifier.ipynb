{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic sign classification  - German Traffic Sign Data\n",
    "### Pytorch Implementation\n",
    "\n",
    "---\n",
    "## Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "training_file = \"data/train.p\"\n",
    "validation_file=\"data/valid.p\"\n",
    "testing_file = \"data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 34799\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "# Number of training examples\n",
    "n_train = len(train[\"features\"])\n",
    "\n",
    "# Number of validation examples\n",
    "n_validation = len(valid[\"features\"])\n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = len(test[\"features\"])\n",
    "\n",
    "# What's the shape of an traffic sign image?\n",
    "image_shape = train[\"features\"][0].shape\n",
    "\n",
    "# How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(np.concatenate((train[\"labels\"],test[\"labels\"],valid[\"labels\"]))))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 30., 240., 240., 150., 210., 210.,  60., 150., 150., 150., 210.,\n",
       "        150., 210., 240.,  90.,  90.,  60., 120., 120.,  30.,  60.,  60.,\n",
       "         60.,  60.,  30., 150.,  60.,  30.,  60.,  30.,  60.,  90.,  30.,\n",
       "         90.,  60., 120.,  60.,  30., 210.,  30.,  60.,  30.,  30.]),\n",
       " array([ 0.        ,  0.97674419,  1.95348837,  2.93023256,  3.90697674,\n",
       "         4.88372093,  5.86046512,  6.8372093 ,  7.81395349,  8.79069767,\n",
       "         9.76744186, 10.74418605, 11.72093023, 12.69767442, 13.6744186 ,\n",
       "        14.65116279, 15.62790698, 16.60465116, 17.58139535, 18.55813953,\n",
       "        19.53488372, 20.51162791, 21.48837209, 22.46511628, 23.44186047,\n",
       "        24.41860465, 25.39534884, 26.37209302, 27.34883721, 28.3255814 ,\n",
       "        29.30232558, 30.27906977, 31.25581395, 32.23255814, 33.20930233,\n",
       "        34.18604651, 35.1627907 , 36.13953488, 37.11627907, 38.09302326,\n",
       "        39.06976744, 40.04651163, 41.02325581, 42.        ]),\n",
       " <a list of 43 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(20,10),sharex=False,sharey=True)\n",
    "axs[0].hist(y_train,bins=43)\n",
    "axs[1].hist(y_test,bins=43)\n",
    "axs[2].hist(y_valid,bins=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device=device,dtype=None)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "device = get_default_device()\n",
    "\n",
    "#Data augmentation and normalization\n",
    "def trainTransform(x):\n",
    "    tfm = tv.transforms.Compose([tv.transforms.ToTensor(),\n",
    "                                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    x = Image.fromarray(x)\n",
    "    x = tfm(x)\n",
    "    return x\n",
    "\n",
    "def brightnessTransform(x):\n",
    "    tfm = tv.transforms.Compose([tv.transforms.ColorJitter(0.5),\n",
    "                                tv.transforms.ToTensor(),\n",
    "                                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    x = Image.fromarray(x)\n",
    "    x = tfm(x)\n",
    "    return x\n",
    "\n",
    "def grayTransform(x):\n",
    "    tfm = tv.transforms.Compose([tv.transforms.RandomGrayscale(p=1),\n",
    "                                tv.transforms.ToTensor(),\n",
    "                                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    x = Image.fromarray(x)\n",
    "    x = tfm(x)\n",
    "    return x\n",
    "\n",
    "def resizeTransform(x):\n",
    "    tfm = tv.transforms.Compose([tv.transforms.RandomResizedCrop(32,scale=(0.07,1)),\n",
    "                                tv.transforms.ToTensor(),\n",
    "                                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    x = Image.fromarray(x)\n",
    "    x = tfm(x)\n",
    "    return x\n",
    "\n",
    "def affineTransform(x):\n",
    "    tfm = tv.transforms.Compose([tv.transforms.RandomAffine(degrees=20,scale=(0.07,1),\n",
    "                                                            translate=(0.08,0.08),shear=(10,10)),\n",
    "                                tv.transforms.ToTensor(),\n",
    "                                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    x = Image.fromarray(x)\n",
    "    x = tfm(x)\n",
    "    return x\n",
    "    \n",
    "trainX = torch.stack([trainTransform(x) for x in X_train])\n",
    "trainX_bright = torch.stack([brightnessTransform(x) for x in X_train])\n",
    "trainX_gray = torch.stack([grayTransform(x) for x in X_train])\n",
    "trainX_resize = torch.stack([resizeTransform(x) for x in X_train])\n",
    "trainX_affine = torch.stack([affineTransform(x) for x in X_train])\n",
    "testX = torch.stack([trainTransform(x) for x in X_test])\n",
    "validX = torch.stack([trainTransform(x) for x in X_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset, ConcatDataset\n",
    "train_base_dataset = TensorDataset(trainX,torch.LongTensor(y_train))\n",
    "train_bright_dataset = TensorDataset(trainX_bright,torch.LongTensor(y_train))\n",
    "train_gray_dataset = TensorDataset(trainX_gray,torch.LongTensor(y_train))\n",
    "train_resize_dataset = TensorDataset(trainX_resize,torch.LongTensor(y_train))\n",
    "train_affine_dataset = TensorDataset(trainX_affine,torch.LongTensor(y_train))\n",
    "\n",
    "train_dataset = ConcatDataset([train_base_dataset,train_bright_dataset,train_gray_dataset,\n",
    "                               train_resize_dataset,train_affine_dataset])\n",
    "test_dataset = TensorDataset(testX,torch.LongTensor(y_test))\n",
    "valid_dataset = TensorDataset(validX,torch.LongTensor(y_valid))\n",
    "\n",
    "#Define dataloaders\n",
    "def get_train_loader(batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "    train_loader = DeviceDataLoader(train_loader,device)\n",
    "    return(train_loader)\n",
    "\n",
    "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size=len(y_test), shuffle=True,num_workers=2),device)\n",
    "val_loader = DeviceDataLoader(DataLoader(valid_dataset, batch_size=len(y_valid), shuffle=True,num_workers=2),device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        #input shape: 32x32x3\n",
    "        #Conv layer 1: Input channels = 3, output channels = 6, filter size = 5x5, stride = 1\n",
    "        self.conv1 = torch.nn.Conv2d(3, 6, kernel_size=(5,5), stride=1,padding=0)\n",
    "        \n",
    "        #shape:28x28x6 \n",
    "        \n",
    "        \n",
    "        #Pool layer 1: filter size = 2x2, stride = 2\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=(2,2), stride=2, padding=0)\n",
    "        \n",
    "        #shape: 14x14x6\n",
    "        \n",
    "        #Conv layer 2: Input channels = 6, output channels = 16, filter size = 5x5, stride = 1\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, kernel_size=(5,5), stride=1,padding=0)\n",
    "        \n",
    "        #shape: 10x10x16\n",
    "        \n",
    "        #Pool layer 2: filter size = 2x2, stride = 2\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=(2,2), stride=2, padding=0)\n",
    "        \n",
    "        #shape: 5x5x16\n",
    "        \n",
    "        #self.flatten1 = torch.flatten()\n",
    "        \n",
    "        #Fully connected layer 1: 400 input features, 120 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(400, 120)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "\n",
    "        #Fully connected layer 2: 120 input features, 10 output features for our 10 defined classes\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        #self.dropout = torch.nn.Dropout(p=0.)\n",
    "        \n",
    "        #Output layer: Softmax, 43 outputs\n",
    "        #self.fc3 = torch.nn.Linear(120,84)\n",
    "        \n",
    "        self.fc3 = torch.nn.Linear(84,43)\n",
    "        self.softmaxout = torch.nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 400)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmaxout(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate=0.001):\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, (inputs,labels) in enumerate(train_loader):\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs,labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #statistics\n",
    "            running_loss += loss_size.data\n",
    "            total_train_loss += loss_size.data\n",
    "            pred = outputs.detach().max(1)[1]\n",
    "            correct += pred.eq(labels.view_as(pred)).sum()\n",
    "            \n",
    "            #Print every epoch\n",
    "            if i+1 == n_batches:\n",
    "                print(\"Epoch {} \\ntraining accuracy: {:.2f} took: {:.2f}s\".format(\n",
    "                    epoch+1, 100*(float(correct) / len(train_loader.dl.dataset)), time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                correct = 0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        total_correct = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            \n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data\n",
    "            pred = val_outputs.detach().max(1)[1]\n",
    "            total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "        \n",
    "        print(\"Validation accuracy = {:.2f}%\".format(100*(float(total_correct) / len(val_loader.dl.dataset))))\n",
    "        print(\"Validation loss = {:.2f}\".format(float(total_val_loss) / len(val_loader.dl.dataset)))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 256\n",
      "epochs= 10\n",
      "learning_rate= 0.001\n",
      "==============================\n",
      "Epoch 1 \n",
      "training accuracy: 40.62 took: 8.73s\n",
      "Validation accuracy = 76.67%\n",
      "Validation loss = 0.00\n",
      "Epoch 2 \n",
      "training accuracy: 68.37 took: 9.82s\n",
      "Validation accuracy = 87.30%\n",
      "Validation loss = 0.00\n",
      "Epoch 3 \n",
      "training accuracy: 73.18 took: 9.60s\n",
      "Validation accuracy = 89.80%\n",
      "Validation loss = 0.00\n",
      "Epoch 4 \n",
      "training accuracy: 75.54 took: 7.95s\n",
      "Validation accuracy = 91.13%\n",
      "Validation loss = 0.00\n",
      "Epoch 5 \n",
      "training accuracy: 77.14 took: 8.12s\n",
      "Validation accuracy = 91.93%\n",
      "Validation loss = 0.00\n",
      "Epoch 6 \n",
      "training accuracy: 78.34 took: 7.86s\n",
      "Validation accuracy = 93.20%\n",
      "Validation loss = 0.00\n",
      "Epoch 7 \n",
      "training accuracy: 79.32 took: 7.71s\n",
      "Validation accuracy = 92.09%\n",
      "Validation loss = 0.00\n",
      "Epoch 8 \n",
      "training accuracy: 80.03 took: 8.22s\n",
      "Validation accuracy = 93.99%\n",
      "Validation loss = 0.00\n",
      "Epoch 9 \n",
      "training accuracy: 80.67 took: 8.12s\n",
      "Validation accuracy = 93.11%\n",
      "Validation loss = 0.00\n",
      "Epoch 10 \n",
      "training accuracy: 81.17 took: 8.09s\n",
      "Validation accuracy = 94.44%\n",
      "Validation loss = 0.00\n",
      "Training finished, took 92.97s\n"
     ]
    }
   ],
   "source": [
    "CNN = LeNet()\n",
    "to_device(CNN,device)\n",
    "trainNet(CNN, batch_size=256, n_epochs=10, learning_rate=0.001)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
